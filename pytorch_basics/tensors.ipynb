{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4) # allocates memory for tensor; values are the values in memory at time of allocation\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.2332, 0.4047, 0.2162],\n",
      "         [0.9927, 0.4128, 0.5938]],\n",
      "\n",
      "        [[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.*_like() produces a tensor with the same shape as the input tensor\n",
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor() creates a copy of the data\n",
    "\n",
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)\n",
    "\n",
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[10.8626,  2.1505, 19.6913],\n",
      "        [ 0.9956,  1.4148,  5.8364]], dtype=torch.float64)\n",
      "tensor([[10,  2, 19],\n",
      "        [ 0,  1,  5]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32) # truncates float to int\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "# Basic arithmetic\n",
    "\n",
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# run-time error\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 2)\n",
    "\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7357, 0.0381, 0.2138, 0.5395],\n",
      "        [0.3686, 0.4007, 0.7220, 0.8217]])\n",
      "tensor([[1.4715, 0.0762, 0.4276, 1.0791],\n",
      "        [0.7371, 0.8014, 1.4439, 1.6434]])\n"
     ]
    }
   ],
   "source": [
    "# broadcasting\n",
    "\n",
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Brad Heintz's explanation)\n",
    "\n",
    "The rules for broadcasting are:\n",
    "\n",
    "Each tensor must have at least one dimension - no empty tensors.\n",
    "\n",
    "Comparing the dimension sizes of the two tensors, going from last to first:\n",
    "\n",
    "Each dimension must be equal, or\n",
    "\n",
    "One of the dimensions must be of size 1, or\n",
    "\n",
    "The dimension does not exist in one of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2612, 0.7375],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.2941, 0.3788]],\n",
      "\n",
      "        [[0.2612, 0.7375],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.2941, 0.3788]],\n",
      "\n",
      "        [[0.2612, 0.7375],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.2941, 0.3788]],\n",
      "\n",
      "        [[0.2612, 0.7375],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.2941, 0.3788]]])\n",
      "tensor([[[0.4567, 0.4567],\n",
      "         [0.0649, 0.0649],\n",
      "         [0.6677, 0.6677]],\n",
      "\n",
      "        [[0.4567, 0.4567],\n",
      "         [0.0649, 0.0649],\n",
      "         [0.6677, 0.6677]],\n",
      "\n",
      "        [[0.4567, 0.4567],\n",
      "         [0.0649, 0.0649],\n",
      "         [0.6677, 0.6677]],\n",
      "\n",
      "        [[0.4567, 0.4567],\n",
      "         [0.0649, 0.0649],\n",
      "         [0.6677, 0.6677]]])\n",
      "tensor([[[0.7826, 0.1332],\n",
      "         [0.7826, 0.1332],\n",
      "         [0.7826, 0.1332]],\n",
      "\n",
      "        [[0.7826, 0.1332],\n",
      "         [0.7826, 0.1332],\n",
      "         [0.7826, 0.1332]],\n",
      "\n",
      "        [[0.7826, 0.1332],\n",
      "         [0.7826, 0.1332],\n",
      "         [0.7826, 0.1332]],\n",
      "\n",
      "        [[0.7826, 0.1332],\n",
      "         [0.7826, 0.1332],\n",
      "         [0.7826, 0.1332]]])\n"
     ]
    }
   ],
   "source": [
    "# valid broadcasting\n",
    "\n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m     torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# dimensions must match last-to-first\u001b[39;00m\n\u001b[1;32m      5\u001b[0m c \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(   \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# both 3rd & 2nd dims different\u001b[39;00m\n\u001b[1;32m      7\u001b[0m d \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m0\u001b[39m, ))   \u001b[38;5;66;03m# can't broadcast with an empty tensor\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# invalid broadcasting\n",
    "\n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
    "\n",
    "c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "\n",
    "d = a * torch.rand((0, ))   # can't broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.3730, 0.2772, 0.2987, 0.4734],\n",
      "        [0.0476, 0.8904, 0.5951, 0.1461]])\n",
      "tensor([[1., -0., 1., -0.],\n",
      "        [-0., -0., -0., 1.]])\n",
      "tensor([[ 0., -1.,  0., -1.],\n",
      "        [-1., -1., -1.,  0.]])\n",
      "tensor([[ 0.3730, -0.2772,  0.2987, -0.4734],\n",
      "        [-0.0476, -0.5000, -0.5000,  0.1461]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.7191, 0.4067],\n",
      "        [0.7301, 0.6276]])\n",
      "tensor([[2.1573, 1.2201],\n",
      "        [2.1903, 1.8827]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.6501, -0.7598],\n",
      "        [-0.7598,  0.6501]]),\n",
      "S=tensor([3.7882, 0.3667]),\n",
      "V=tensor([[-0.8096, -0.5870],\n",
      "        [-0.5870,  0.8096]]))\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "# Altering tensors in place\n",
    "\n",
    "# Most binary operations on tensors return a new tensor with the result of the operation. (e.g. c = a * b creates a new tensor occupying a different region of memory)\n",
    "\n",
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
    "print(a)              # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # note the underscore\n",
    "print(b)              # b has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.7357, 0.0381],\n",
      "        [0.2138, 0.5395]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.7357, 1.0381],\n",
      "        [1.2138, 1.5395]])\n",
      "tensor([[1.7357, 1.0381],\n",
      "        [1.2138, 1.5395]])\n",
      "tensor([[0.7357, 0.0381],\n",
      "        [0.2138, 0.5395]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.5413, 0.0015],\n",
      "        [0.0457, 0.2911]])\n",
      "tensor([[0.5413, 0.0015],\n",
      "        [0.0457, 0.2911]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b)) # a changes, b does not\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b)) # b changes\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Brad Heintz's explanation)\n",
    "There is another option for placing the result of a computation in an existing, allocated tensor. Many of the methods and functions we’ve seen so far - including creation methods! - have an out argument that lets you specify a tensor to receive the output. If the out tensor is the correct shape and dtype, this can happen without a new memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.4300, 0.6102],\n",
      "        [0.8729, 1.2263]])\n",
      "tensor([[0.2941, 0.3788],\n",
      "        [0.4567, 0.0649]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c) == old_id  # still the same object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "# Copying tensors\n",
    "\n",
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# If you want a separate copy, use clone()\n",
    "\n",
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # different objects in memory...\n",
    "print(torch.eq(a, b))  # ...but still with the same contents!\n",
    "\n",
    "a[0][1] = 561          # a changes...\n",
    "print(b)               # ...but b is still all ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6865, 0.3614],\n",
      "        [0.6493, 0.2633]], requires_grad=True)\n",
      "tensor([[0.6865, 0.3614],\n",
      "        [0.6493, 0.2633]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.6865, 0.3614],\n",
      "        [0.6493, 0.2633]])\n",
      "tensor([[0.6865, 0.3614],\n",
      "        [0.6493, 0.2633]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# If your source tensor has autograd on, the clone will too.\n",
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, CPU only.\n"
     ]
    }
   ],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, CPU only.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Brad Heintz's explanation)\n",
    "You can query the number of GPUs with torch.cuda.device_count(). If you have more than one GPU, you can specify them by index: device='cuda:0', device='cuda:1', etc.\n",
    "\n",
    "As a coding practice, specifying our devices everywhere with string constants is pretty fragile. In an ideal world, your code would perform robustly whether you’re on CPU or GPU hardware. You can do this by creating a device handle that can be passed to your tensors instead of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "tensor([[0.4762, 0.0548],\n",
      "        [0.2024, 0.5731]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving a tensor from CPU to GPU\n",
    "y = torch.rand(2, 2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# Changing tensor shapes\n",
    "\n",
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # adds a dimension at index 0\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[3.6907e-01, 7.1114e-04, 4.4247e-01, 2.2991e-01, 5.2522e-01, 9.4127e-01,\n",
      "         9.1769e-01, 1.3804e-01, 2.2586e-02, 3.0251e-01, 1.4478e-01, 5.7584e-01,\n",
      "         8.9923e-01, 2.3469e-01, 1.8992e-01, 4.0671e-01, 1.5190e-01, 1.5065e-01,\n",
      "         9.5853e-01, 7.7563e-01]])\n",
      "torch.Size([20])\n",
      "tensor([3.6907e-01, 7.1114e-04, 4.4247e-01, 2.2991e-01, 5.2522e-01, 9.4127e-01,\n",
      "        9.1769e-01, 1.3804e-01, 2.2586e-02, 3.0251e-01, 1.4478e-01, 5.7584e-01,\n",
      "        8.9923e-01, 2.3469e-01, 1.8992e-01, 4.0671e-01, 1.5190e-01, 1.5065e-01,\n",
      "        9.5853e-01, 7.7563e-01])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape) # doesn't do anything because if it did, it'd change the number of elements in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.4509, 0.4509],\n",
      "         [0.2690, 0.2690],\n",
      "         [0.8381, 0.8381]],\n",
      "\n",
      "        [[0.4509, 0.4509],\n",
      "         [0.2690, 0.2690],\n",
      "         [0.8381, 0.8381]],\n",
      "\n",
      "        [[0.4509, 0.4509],\n",
      "         [0.2690, 0.2690],\n",
      "         [0.8381, 0.8381]],\n",
      "\n",
      "        [[0.4509, 0.4509],\n",
      "         [0.2690, 0.2690],\n",
      "         [0.8381, 0.8381]]])\n"
     ]
    }
   ],
   "source": [
    "# Applying unsqueeze to broadcasting\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# In-place versions of squeeze and unsqueeze\n",
    "\n",
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Brad Heintz's explanation)\n",
    "When it can, reshape() will return a view on the tensor to be changed - that is, a separate tensor object looking at the same underlying region of memory. This is important: That means any change made to the source tensor will be reflected in the view on that tensor, unless you clone() it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "# Changing shapes more radically, retaining the same number of elements\n",
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[0.4207, 0.8220, 0.2617],\n",
      "        [0.1458, 0.7102, 0.2471]])\n",
      "[[0.42066115 0.82200915 0.2616781 ]\n",
      " [0.14582425 0.710195   0.24705058]]\n"
     ]
    }
   ],
   "source": [
    "# NumPy Bridge\n",
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Brad Heintz's explanation)\n",
    "It is important to know that these converted objects are using the same underlying memory as their source objects, meaning that changes to one are reflected in the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.42066115  0.82200915  0.2616781 ]\n",
      " [ 0.14582425 17.          0.24705058]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1, 1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1, 1] = 17\n",
    "print(numpy_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
