{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0401,  0.0542, -0.0618,  ...,  0.0447,  0.0945, -0.0045],\n",
      "        [-0.0169,  0.0734, -0.0650,  ..., -0.0765,  0.0857,  0.0466],\n",
      "        [-0.0149, -0.0215, -0.0653,  ..., -0.0998,  0.0877,  0.0573],\n",
      "        ...,\n",
      "        [ 0.0544,  0.0401, -0.0527,  ..., -0.0015,  0.0106,  0.0406],\n",
      "        [-0.0898,  0.0129,  0.0853,  ..., -0.0466,  0.0042, -0.0108],\n",
      "        [-0.0132, -0.0213, -0.0277,  ..., -0.0909,  0.0525, -0.0114]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0038,  0.0304, -0.0014,  0.0410,  0.0029,  0.0830,  0.0047,  0.0546,\n",
      "        -0.0904,  0.0124, -0.0101, -0.0458, -0.0225,  0.0919, -0.0443, -0.0590,\n",
      "         0.0711,  0.0415, -0.0243,  0.0487,  0.0507, -0.0122, -0.0522, -0.0350,\n",
      "        -0.0555,  0.0976, -0.0485,  0.0824, -0.0310, -0.0702, -0.0924, -0.0447,\n",
      "        -0.0321, -0.0443,  0.0502, -0.0437,  0.0582,  0.0874, -0.0118, -0.0935,\n",
      "        -0.0967, -0.0538,  0.0286, -0.0357, -0.0238, -0.0104,  0.0466, -0.0178,\n",
      "        -0.0590,  0.0962, -0.0192,  0.0775, -0.0040,  0.0210,  0.0717, -0.0509,\n",
      "         0.0690, -0.0385, -0.0517, -0.0420, -0.0439, -0.0973,  0.0424,  0.0769,\n",
      "         0.0698, -0.0745,  0.0677, -0.0965,  0.0392,  0.0076,  0.0393,  0.0842,\n",
      "        -0.0721,  0.0070,  0.0967,  0.0121,  0.0149,  0.0821, -0.0746,  0.0184,\n",
      "         0.0092,  0.0076,  0.0768,  0.0327,  0.0819,  0.0607,  0.0990, -0.0540,\n",
      "        -0.0141, -0.0223, -0.0802, -0.0954, -0.0212, -0.0602,  0.0624,  0.0973,\n",
      "         0.0491,  0.0601, -0.0126,  0.0371, -0.0814, -0.0633, -0.0575, -0.0615,\n",
      "        -0.0651, -0.0980, -0.0004, -0.0669,  0.0757,  0.0390, -0.0120, -0.0585,\n",
      "         0.0395,  0.0607, -0.0592, -0.0757,  0.0330,  0.0618, -0.0128,  0.0356,\n",
      "         0.0524, -0.0884, -0.0499, -0.0028,  0.0681, -0.0974, -0.0292,  0.0393,\n",
      "         0.0504, -0.0868,  0.0240,  0.0520, -0.0440,  0.0189, -0.0185, -0.0968,\n",
      "         0.0655, -0.0052,  0.0040,  0.0202, -0.0399, -0.0909, -0.0107,  0.0648,\n",
      "         0.0933, -0.0613, -0.0041,  0.0162, -0.0953, -0.0457,  0.0014,  0.0041,\n",
      "         0.0236, -0.0528,  0.0794, -0.0346, -0.0556, -0.0201, -0.0280, -0.0283,\n",
      "         0.0397,  0.0528,  0.0991, -0.0385, -0.0772, -0.0043,  0.0461, -0.0777,\n",
      "        -0.0226, -0.0640, -0.0370, -0.0091,  0.0401, -0.0557,  0.0856,  0.0041,\n",
      "         0.0444,  0.0686,  0.0644, -0.0305,  0.0361, -0.0313, -0.0876, -0.0192,\n",
      "        -0.0354,  0.0183,  0.0093,  0.0775,  0.0344,  0.0204,  0.0050,  0.0731,\n",
      "        -0.0481, -0.0934, -0.0536, -0.0401,  0.0687,  0.0943, -0.0231, -0.0917],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0259, -0.0699,  0.0122,  ...,  0.0140,  0.0177, -0.0052],\n",
      "        [ 0.0448,  0.0060,  0.0303,  ...,  0.0028,  0.0692, -0.0021],\n",
      "        [ 0.0435, -0.0405, -0.0350,  ...,  0.0438, -0.0388, -0.0549],\n",
      "        ...,\n",
      "        [-0.0261, -0.0622, -0.0188,  ..., -0.0663, -0.0370, -0.0521],\n",
      "        [-0.0687,  0.0537, -0.0134,  ...,  0.0607, -0.0110,  0.0412],\n",
      "        [-0.0458, -0.0254, -0.0317,  ...,  0.0110, -0.0209, -0.0164]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0325,  0.0295,  0.0057, -0.0012, -0.0458, -0.0410,  0.0179, -0.0478,\n",
      "         0.0114, -0.0170], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0259, -0.0699,  0.0122,  ...,  0.0140,  0.0177, -0.0052],\n",
      "        [ 0.0448,  0.0060,  0.0303,  ...,  0.0028,  0.0692, -0.0021],\n",
      "        [ 0.0435, -0.0405, -0.0350,  ...,  0.0438, -0.0388, -0.0549],\n",
      "        ...,\n",
      "        [-0.0261, -0.0622, -0.0188,  ..., -0.0663, -0.0370, -0.0521],\n",
      "        [-0.0687,  0.0537, -0.0134,  ...,  0.0607, -0.0110,  0.0412],\n",
      "        [-0.0458, -0.0254, -0.0317,  ...,  0.0110, -0.0209, -0.0164]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0325,  0.0295,  0.0057, -0.0012, -0.0458, -0.0410,  0.0179, -0.0478,\n",
      "         0.0114, -0.0170], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.2714, 0.5355, 0.0601]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.5156,  0.4255, -0.4125],\n",
      "        [-0.0899, -0.3691,  0.0729]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4603, -0.4347], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[-0.3972, -0.6525]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Linear layer\n",
    "\n",
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "\n",
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0704, 0.0677, 0.4209, 0.9724, 0.9198, 0.7782],\n",
      "         [0.4635, 0.6394, 0.6265, 0.9837, 0.7898, 0.7967],\n",
      "         [0.5524, 0.5817, 0.5782, 0.9593, 0.2666, 0.1690],\n",
      "         [0.2643, 0.2477, 0.1438, 0.7373, 0.6290, 0.7404],\n",
      "         [0.8417, 0.9429, 0.5725, 0.6315, 0.3002, 0.2141],\n",
      "         [0.5314, 0.0663, 0.8556, 0.4556, 0.6065, 0.3402]]])\n",
      "tensor([[[0.6394, 0.9837],\n",
      "         [0.9429, 0.7404]]])\n"
     ]
    }
   ],
   "source": [
    "# Other non-learning layers\n",
    "# Max pooling\n",
    "\n",
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[22.8049, 18.2147,  9.3425,  7.0408],\n",
      "         [12.6681, 14.5861, 23.6404,  6.0753],\n",
      "         [14.8111, 20.0515, 15.3228, 11.3324],\n",
      "         [ 5.1860, 23.2855,  8.1309, 19.3826]]])\n",
      "tensor(14.4922)\n",
      "tensor([[[ 1.3166,  0.6018, -0.7800, -1.1384],\n",
      "         [-0.2508,  0.0547,  1.4971, -1.3010],\n",
      "         [-0.1831,  1.5053, -0.0182, -1.3039],\n",
      "         [-1.1687,  1.2322, -0.7780,  0.7145]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(-2.9802e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalization layers\n",
    "\n",
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8480, 1.1984, 0.0000, 0.7435],\n",
      "         [0.0000, 0.1335, 0.0000, 0.5203],\n",
      "         [0.4302, 0.0000, 1.2287, 0.4959],\n",
      "         [1.6579, 0.0000, 0.0000, 0.0000]]])\n",
      "tensor([[[0.8480, 1.1984, 0.0000, 0.7435],\n",
      "         [0.0000, 0.1335, 1.5447, 0.5203],\n",
      "         [0.0000, 0.0145, 0.0000, 0.4959],\n",
      "         [0.0000, 1.1870, 0.6534, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "# Dropout layer\n",
    "\n",
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Module has many major activation functions and loss functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
